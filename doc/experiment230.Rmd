## Experiment 3: The Effect of The Number of CPU Cores

### Experimental Setup

I execute the hierarchical clustering, changing the some parameters as bellow. 
The data had been generated randomly. 
And then I measured the execution time for training each model.

- # Used Cores: 10, 20, 30, 50, 100, 160
- # Clusters: 20
- # Rows: 1000000, 5000000
- # Dimensions: 1000

### The Result of Training Execution Time

```{r echo=FALSE, warning=FALSE}
library(reshape2)
result4 <- read.csv("./data/benchmark-cpu-cores.csv")
result4$sec <- result4$trainMilliSec / 1000
```

```{r echo=FALSE, warning=FALSE}
result4.cast <- dcast(result4, maxCores ~ rows, value.var="sec", sum)
x <- result4.cast[, 1]
y <- result4.cast[, 2:ncol(result4.cast)]
.names <- names(y)
matplot(x, y
        , xlab="# CPU Cores"
        , ylab="Training Execution Time [sec]"
        , ylim=c(0, max(y))
        , pch=1:(length(y)), col=rainbow(length(y)), type="o")
grid()
legend("topright", legend=.names
       , pch=1:(length(y)), col=rainbow(length(y)))

x <- result4.cast[, 1]
y <- apply(result4.cast[, 2:ncol(result4.cast)], 2, function(x){x / x[1]})
.xlab <- "# CPU Cores"
.ylab <- paste("Index against # Cores = ", min(result4$maxCores), sep='')
matplot(x, y
        , xlab=.xlab , ylab=.ylab
        , ylim=c(0, max(y))
        , pch=1:(length(y)), col=rainbow(ncol(y)), type="o")
grid()
legend("topright", legend=.names
       , pch=1:(length(y)), col=rainbow(ncol(y)))
```


```{r echo=FALSE, warning=FALSE, results="asis"}
kable(result4)
```
